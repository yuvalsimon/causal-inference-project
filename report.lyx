#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 4cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Online Article's Popularity
\end_layout

\begin_layout Author
Yuval Simon
\begin_inset Newline newline
\end_inset

Ifat Peczenik
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Digital content consumption has grown rapidly over the years as people are
 spending more and more time reading online content.
 Together with the growing number of users, also the competition between
 online media platforms has rapidly increased.
 Online media platforms like Medium, Mashable and Buzzfeed publish hundreds
 of articles every day, aiming to bring the best content to the users and
 bring most shares.
 
\end_layout

\begin_layout Standard
Article's popularity can be estimated by its number of shares.
 In this project we will try to estimate the causal effect of publishing
 an article during weekend on its popularity, which is measured by its number
 of shares.
 There are some confounders that can affect the popularity of an article
 and we take them into account in our estimations, for example some readers
 might not have time to read a very long article, or the title of an article
 might be less distracting even though the article is great.
 Many confounders are hidden, but we'll try to disable some of them where
 it is possible.
\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Standard
Our data contains around 39000 articles that were published in Mashable
 between 07/01/2013 and 27/12/2014.
 
\end_layout

\begin_layout Standard
For each article the data contains many features that could have also been
 extracted from the article's url, for instance:
\end_layout

\begin_layout Itemize
Publish date
\end_layout

\begin_layout Itemize
Data channel: business, entertainment, social media, world, technology,
 lifestyle
\end_layout

\begin_layout Itemize
Number of images, videos, links
\end_layout

\begin_layout Itemize
Article's length: number of words in the title/content
\end_layout

\begin_layout Itemize
Number of keywords
\end_layout

\begin_layout Itemize
Rate of positive and negative words
\end_layout

\begin_layout Itemize
Title and content subjectivity level 
\end_layout

\begin_layout Itemize
Max, min and average polarity of positive and negative word
\end_layout

\begin_layout Itemize
Published on weekend or not 
\end_layout

\begin_layout Itemize
Number of shares
\end_layout

\begin_layout Standard
Some features in the dataset, like word polarity and subjectivity level,
 are features that were calculated with some method unknown to us.
 We take it as-is and we assume the dataset author exploited good methods
 to calculate them.
\end_layout

\begin_layout Standard
In addition there are some hidden confounders we need to address:
\end_layout

\begin_layout Itemize
Audience - we don't know the characteristics of people that enter to the
 website, and specifically to each of the channels in Mashable.
\end_layout

\begin_layout Itemize
Promotion algorithms - some articles are promoted at main view of Mashable.
 In 2013-2014 there were 3 promotion areas - 
\begin_inset Quotes eld
\end_inset

The New Stuff
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

The Next Big Thing
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

What's Hot
\begin_inset Quotes erd
\end_inset

.
 We don't know which articles were promoted and how much could increase
 the chances of an article to get shared more.
\end_layout

\begin_layout Itemize
Advertising - some articles might had been advertised in other websites,
 like Facebook and Google.
\end_layout

\begin_layout Itemize
Special occasions - some articles might had been published after some special
 occasion.
 For example, an article about Apple that is published after Apple reveals
 its new Iphone may get more exposure.
\end_layout

\begin_layout Standard
We assume a few things on the data that will allow us to estimate the causal
 effect:
\end_layout

\begin_layout Enumerate
Stable unit treatment value assumption (SUTVA) - the potential outcomes
 (number of shares) for any article do not vary significantly with the treatment
 (whether published on weekend) assigned to the other articles.
 It probably does vary, for example if a promoted article may have an effect
 on the number of people reading a regular article on the day it is published.
 We assume this variation is negligible.
 Moreover, we assume that the treatment is binary, there is the effect of
 the specific date at which an article was published on its number of shares
 is neglibile.
 There may be specifal occasions that occured on some dates but we assume
 its effect on article's popularity is negligible.
 We drop from our calculations articles that were published on holidays.
\end_layout

\begin_layout Enumerate
We assume that an article's popularity is represented exactly by the number
 of shares of the article.
 Our data contains the number of shares of all the articles so consistency
 holds.
\end_layout

\begin_layout Enumerate
Ignorability - we assume that the effect of the hidden confounders is negligible
 or have a causal effect on the outcome only through the treatment.
 We assume that all the signifcant confounders are measured.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/corr_matrix.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Correlation between features
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Data distributions
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/weekend_count.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Number of articles published on weekend
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/data_channel_count.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Number of articles per data channel
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/num_shares_percentile_97.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Histogram of percentile 0.97 of shares count
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Method
\end_layout

\begin_layout Standard
We tried to estimate the average treatment effect (ATE) using a few methods:
 Inverse probability weighting (IPW), Matching, S learner, T learner and
 X learner.
 In order to estimate ATE we need that common support will hold.
 We calculated propensity scores (
\begin_inset Formula $e\left(x\right)=p\left(T=1\mid x\right)$
\end_inset

) in a few methods and eliminated some articles using the propensity histogram
 in order for common support to hold.
 We'll go into details in the following sections.
\end_layout

\begin_layout Subsubsection*
Propensity estimation
\end_layout

\begin_layout Standard
We estimated propensity scores with 3 methods:
\end_layout

\begin_layout Enumerate
Logistic regression: it had 
\begin_inset Formula $AUROC=0.596$
\end_inset

, which is good (close to 0.5) because it means the model is not predicting
 the treatment based on the confounders which would have violated common
 support.
 The calibration curve is mostly good - it turns bad in the graphs' edges,
 where there are very few samples.
 We trimmed the propensity histogram in order to have common support.
 (Figure 3)
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_normal.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_calibration.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_trimmed.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_trimmed_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Random forest: it had 
\begin_inset Formula $AUROC=0.7$
\end_inset

, which is good.
 In order to prevent overfitting we set hypermeters max tree depth to 7
 and minimum leaf samples to 40.
 The calibration curve is worse than the other methods.
 
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_normal.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_calibration.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_trimmed.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_trimmed_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Gradient boosting: it had 
\begin_inset Formula $AUROC=0.69$
\end_inset

, which is good.
 In order to prevent overfitting we set maximum tree depth to 3, minimum
 leaf samples to 30 and number of estimators to 100 .
 The calibration curve is quite good.
 It turns bad in the areas with less samples, but we trim these areas
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_normal.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_calibration.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_trimmed.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_trimmed_log.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We used recursive feature elimination (RFE) in order to find the most important
 features.
 We can see (figure 4) that logistic regression gives more wieght to the
 sentiment, subjectivity of the article and the ratio of positive and negative
 words, while random forest and gradient boosting give more weight to the
 length of the article, the number of elements of each type and the sentiment.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Feature importance rankings
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/log.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression model
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/random_forest.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Random forest model
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/boosting.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Gradient boosting model
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
ATE estimation
\end_layout

\begin_layout Standard
We tried to estimate ATE using a couple of methods, together with each of
 the above propensity methods.
 
\end_layout

\begin_layout Enumerate
Inverse probability weighting (IPW):
\end_layout

\begin_layout Enumerate
Matching:
\end_layout

\begin_layout Enumerate
S learner
\end_layout

\begin_layout Enumerate
T learner
\end_layout

\begin_layout Enumerate
X learner
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ATE estimates
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/logistic_regression_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With logistic regression propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/random_forest_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With random forest propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/boosting_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With boosting propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Inverse probability weighting (IPW) with propensity scores- we use 3 different
 methods to estimate propensity scores: logistic regression, random forest
 and gradient boosting.
 For each method we calculate AUROC indicator and check for common support
 and calibration.
 In order to achieve better common support overlapping, we used trimming
 to remove outliers propensity scores from our model.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Common support (overlap)- we assume that every unit in our dataset has probabili
ty greater than 0 to get T=0 or T=1.
 Plot histogram of propensity scores: 
\end_layout

\end_deeper
\begin_layout Itemize
Matching- for each article with confounders X and treatment T we find the
 closest article that got treatment 1-T.
 Optimal unit matching that we use is minimizing sum of distances between
 all pairs.
 After finding for each unit its match, we calculate individual treatment
 effect (ITE) for every unit and finally calculate the ATE.
 
\end_layout

\begin_layout Itemize
S learner- models Y(0) and Y(1) through one model that receives the treatment
 assignment T as an input feature, along with the features X.
 Here we used one linear regression model to estimate y~f(x,t).
 Then we predict ATE on all the data.
\end_layout

\begin_layout Itemize
T learner- models Y(0) and Y(1) separately using two different models.
 Here we used 2 linear regression models to estimate y0~f0(x) and y1~f1(x).
 Then we predict ATE on all the data.
\end_layout

\begin_layout Itemize
X learner- in our dataset there are many units with T=0 (~34k samples) and
 less with T=1 (~5k samples), therefore we chose to use X learner model
 to overcome sample size difference.
 Here we models Y(0) and Y(1) separately as we did in T learner, using 2
 linear regression models to estimate y0~f0(x) and y1~f1(x).
 Now we estimate conditional average treatment effect on the treated and
 conditional average treatment effect on the controls using random forest
 regressors.
 Then we estimate CATE using weighting function- here we use propensity
 score estimation.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Weaknesses
\end_layout

\begin_layout Itemize
In our dataset there are only 5190 articles published on weekend.
 We need that common support will hold, so we are very limited with the
 number of confounders we can consider in order to calculate ATE.
 Having more articles published on weekend could potentially change our
 results and impact our conclusions.
 
\end_layout

\begin_layout Itemize
We may have hidden confounders that are impacting only outcome Y, number
 of shares.
 We assumed that all hidden confounders are impacting Y only through T and
 by that our data is satisfying ignorability.
 However, this asssumption can't be verifiable from the data itself.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
target "https://www.kaggle.com/srikaranelakurthy/online-news-popularity"

\end_inset


\end_layout

\end_body
\end_document
