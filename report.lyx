#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Online Article's Popularity
\end_layout

\begin_layout Author
Yuval Simon
\begin_inset Newline newline
\end_inset

Ifat Peczenik
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Digital content consumption has grown rapidly over the years as people are
 spending more and more time reading online content.
 Together with the growing number of users, also the competition between
 online media platforms has rapidly increased.
 Online media platforms like Medium, Mashable and Buzzfeed publish hundreds
 of articles every day, aiming to bring the best content to the users and
 bring most shares.
 
\end_layout

\begin_layout Standard
Article's popularity can be estimated by its number of shares.
 In this project we will try to estimate the causal effect of publishing
 an article during weekend on its popularity, which is measured by its number
 of shares.
 There are some confounders that can affect the popularity of an article
 and we take them into account in our estimations, for example some readers
 might not have time to read a very long article, or the title of an article
 might be less distracting even though the article is great.
 Many confounders are hidden, but we'll try to disable some of them where
 it is possible.
\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Standard
Our data contains around 39000 articles that were published in Mashable
 between 07/01/2013 and 27/12/2014.
 
\end_layout

\begin_layout Standard
For each article the data contains many features that could have also been
 extracted from the article's url, for instance:
\end_layout

\begin_layout Itemize
Publish date
\end_layout

\begin_layout Itemize
Data channel: business, entertainment, social media, world, technology,
 lifestyle
\end_layout

\begin_layout Itemize
Number of images, videos, links
\end_layout

\begin_layout Itemize
Article's length: number of words in the title/content
\end_layout

\begin_layout Itemize
Number of keywords
\end_layout

\begin_layout Itemize
Rate of positive and negative words
\end_layout

\begin_layout Itemize
Title and content subjectivity level 
\end_layout

\begin_layout Itemize
Max, min and average polarity of positive and negative word
\end_layout

\begin_layout Itemize
Published on weekend or not 
\end_layout

\begin_layout Itemize
Number of shares
\end_layout

\begin_layout Standard
Some features in the dataset, like word polarity and subjectivity level,
 are features that were calculated with some method unknown to us.
 We take it as-is and we assume the dataset author exploited good methods
 to calculate them.
\end_layout

\begin_layout Standard
In addition there are some hidden confounders we need to address:
\end_layout

\begin_layout Itemize
Audience - we don't know the characteristics of people that enter to the
 website, and specifically to each of the channels in Mashable.
\end_layout

\begin_layout Itemize
Promotion algorithms - some articles are promoted at main view of Mashable.
 In 2013-2014 there were 3 promotion areas - 
\begin_inset Quotes eld
\end_inset

The New Stuff
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

The Next Big Thing
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

What's Hot
\begin_inset Quotes erd
\end_inset

.
 We don't know which articles were promoted and how much could increase
 the chances of an article to get shared more.
\end_layout

\begin_layout Itemize
Advertising - some articles might had been advertised in other websites,
 like Facebook and Google.
\end_layout

\begin_layout Itemize
Special occasions - some articles might had been published after some special
 occasion.
 For example, an article about Apple that is published after Apple reveals
 its new Iphone may get more exposure.
\end_layout

\begin_layout Standard
We assume a few things on the data that will allow us to estimate the causal
 effect:
\end_layout

\begin_layout Enumerate
Stable unit treatment value assumption (SUTVA) - the potential outcomes
 (number of shares) for any article do not vary significantly with the treatment
 (whether published on weekend) assigned to the other articles.
 It probably does vary, for example if a promoted article may have an effect
 on the number of people reading a regular article on the day it is published.
 We assume this variation is negligible.
 Moreover, we assume that the treatment is binary, there is the effect of
 the specific date at which an article was published on its number of shares
 is neglibile.
 There may be specifal occasions that occured on some dates but we assume
 its effect on article's popularity is negligible.
 We drop from our calculations articles that were published on holidays.
\end_layout

\begin_layout Enumerate
We assume that an article's popularity is represented exactly by the number
 of shares of the article.
 Our data contains the number of shares of all the articles so consistency
 holds.
\end_layout

\begin_layout Enumerate
Ignorability - we assume that the effect of the hidden confounders is negligible.
 All the signifcant confounders are measured.
\end_layout

\begin_layout Enumerate
Ignorability- no unmeasured confounders.
 In other words, two observations with the same set of covariates X but
 with different treatment assignments can be compared to estimate the causal
 effect of the treatment for these observations.
 Here we assume that in our dataset all hidden confounders that affect the
 treatment have no effect on the outcome Y (number of shares), except through
 T (published in weekday or weekend), although this assumption is unverifiable
 from the data itself.
 
\end_layout

\begin_layout Subsection*
Correlation between features:
\end_layout

\begin_layout Subsection*
\begin_inset Graphics
	filename images/corr_matrix.png

\end_inset


\end_layout

\begin_layout Subsection*
Shares histogram:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename images/shares_hist.PNG
	clip

\end_inset


\end_layout

\begin_layout Subsection*
Data channel distribution:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename images/data_channel_dis.PNG

\end_inset


\end_layout

\begin_layout Subsection*
Logistic Regression feature importance:
\end_layout

\begin_layout Section
Method
\end_layout

\begin_layout Standard
We try to estimate average treatment effect using the following causal inference
 methods:
\end_layout

\begin_layout Itemize
Inverse probability weighting (IPW) with propensity scores- we use 3 different
 methods to estimate propensity scores: logistic regression, random forest
 and gradient boosting.
 For each method we calculate AUROC indicator and check for common support
 and calibration.
 In order to achieve better common support overlapping, we used trimming
 to remove outliers propensity scores from our model.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Common support (overlap)- we assume that every unit in our dataset has probabili
ty greater than 0 to get T=0 or T=1.
 Plot histogram of propensity scores: 
\end_layout

\end_deeper
\begin_layout Itemize
Matching- for each article with confounders X and treatment T we find the
 closest article that got treatment 1-T.
 Optimal unit matching that we use is minimizing sum of distances between
 all pairs.
 After finding for each unit its match, we calculate individual treatment
 effect (ITE) for every unit and finally calculate the ATE.
 
\end_layout

\begin_layout Itemize
S learner- models Y(0) and Y(1) through one model that receives the treatment
 assignment T as an input feature, along with the features X.
 Here we used one linear regression model to estimate y~f(x,t).
 Then we predict ATE on all the data.
\end_layout

\begin_layout Itemize
T learner- models Y(0) and Y(1) separately using two different models.
 Here we used 2 linear regression models to estimate y0~f0(x) and y1~f1(x).
 Then we predict ATE on all the data.
\end_layout

\begin_layout Itemize
X learner- in our dataset there are many units with T=0 (~34k samples) and
 less with T=1 (~5k samples), therefore we chose to use X learner model
 to overcome sample size difference.
 Here we models Y(0) and Y(1) separately as we did in T learner, using 2
 linear regression models to estimate y0~f0(x) and y1~f1(x).
 Now we estimate conditional average treatment effect on the treated and
 conditional average treatment effect on the controls using random forest
 regressors.
 Then we estimate CATE using weighting function- here we use propensity
 score estimation.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Weaknesses
\end_layout

\begin_layout Itemize
In our dataset there are only 5190 articles published on weekend.
 We need that common support will hold, so we are very limited with the
 number of confounders we can consider in order to calculate ATE.
 Having more articles published on weekend could potentially change our
 results and impact our conclusions.
 
\end_layout

\begin_layout Itemize
We may have hidden confounders that are impacting only outcome Y, number
 of shares.
 We assumed that all hidden confounders are impacting Y only through T and
 by that our data is satisfying ignorability.
 However, this asssumption can't be verifiable from the data itself.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
target "https://www.kaggle.com/srikaranelakurthy/online-news-popularity"

\end_inset


\end_layout

\end_body
\end_document
