#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 4cm
\rightmargin 4cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Digital Articles Popularity
\end_layout

\begin_layout Author
Yuval Simon
\begin_inset Newline newline
\end_inset

Ifat Peczenik
\end_layout

\begin_layout Date

\size small
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/yuvalsimon/causal-inference-project"
literal "false"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Digital content consumption has grown rapidly over the years as people are
 spending more and more time reading online content.
 Together with the growing number of users, also the competition between
 online media platforms has rapidly increased.
 Online media platforms like Medium, Mashable and Buzzfeed publish hundreds
 of articles every day, aiming to bring the best content to the users and
 bring most shares.
 
\end_layout

\begin_layout Standard
Article's popularity can be estimated by its number of shares.
 In this project we will try to estimate the causal effect of publishing
 an article during weekend on its popularity, which is measured by its number
 of shares.
 There are some confounders that can affect the popularity of an article
 and we take them into account in our estimations, for example some readers
 might not have time to read a very long article, or the title of an article
 might be less distracting even though the article is great.
 Many confounders are hidden, but we'll try to disable some of them where
 it is possible.
\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Standard
Our data
\begin_inset Foot
status open

\begin_layout Author
\begin_inset CommandInset href
LatexCommand href
target "https://www.kaggle.com/srikaranelakurthy/online-news-popularity"

\end_inset


\end_layout

\end_inset

 contains around 39000 articles that were published in Mashable between
 07/01/2013 and 27/12/2014.
 
\end_layout

\begin_layout Standard
For each article the data contains many features that could have also been
 extracted from the article's url, for instance:
\end_layout

\begin_layout Itemize
Publish date
\end_layout

\begin_layout Itemize
Data channel: business, entertainment, social media, world, technology,
 lifestyle
\end_layout

\begin_layout Itemize
Number of images, videos, links
\end_layout

\begin_layout Itemize
Article's length: number of words in the title/content
\end_layout

\begin_layout Itemize
Number of keywords
\end_layout

\begin_layout Itemize
Rate of positive and negative words
\end_layout

\begin_layout Itemize
Title and content subjectivity level 
\end_layout

\begin_layout Itemize
Max, min and average polarity of positive and negative word
\end_layout

\begin_layout Itemize
Published on weekend or not 
\end_layout

\begin_layout Itemize
Number of shares
\end_layout

\begin_layout Standard
Some features in the dataset, like word polarity and subjectivity level,
 are features that were calculated with some method unknown to us.
 We take it as-is and we assume the dataset author exploited good methods
 to calculate them.
\end_layout

\begin_layout Standard
In addition there are some hidden confounders we need to address:
\end_layout

\begin_layout Itemize
Audience - we don't know the characteristics of people that enter to the
 website, and specifically to each of the channels in Mashable.
\end_layout

\begin_layout Itemize
Promotion algorithms - some articles are promoted at main view of Mashable.
 In 2013-2014 there were 3 promotion areas - 
\begin_inset Quotes eld
\end_inset

The New Stuff
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

The Next Big Thing
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

What's Hot
\begin_inset Quotes erd
\end_inset

.
 We don't know which articles were promoted and how much could increase
 the chances of an article to get shared more.
\end_layout

\begin_layout Itemize
Advertising - some articles might had been advertised in other websites,
 like Facebook and Google.
\end_layout

\begin_layout Itemize
Special occasions - some articles might had been published after some special
 occasion.
 For example, an article about Apple that is published after Apple reveals
 its new Iphone may get more exposure.
\end_layout

\begin_layout Standard
We assume a few things on the data that will allow us to estimate the causal
 effect:
\end_layout

\begin_layout Enumerate
Stable unit treatment value assumption (SUTVA) - the potential outcomes
 (number of shares) for any article do not vary significantly with the treatment
 (whether published on weekend) assigned to the other articles.
 It probably does vary, for example if a promoted article may have an effect
 on the number of people reading a regular article on the day it is published.
 We assume this variation is negligible.
 Moreover, we assume that the treatment is binary, there is the effect of
 the specific date at which an article was published on its number of shares
 is neglibile.
 There may be specifal occasions that occured on some dates but we assume
 its effect on article's popularity is negligible.
 We drop from our calculations articles that were published on holidays.
\end_layout

\begin_layout Enumerate
We assume that an article's popularity is represented exactly by the number
 of shares of the article.
 Our data contains the number of shares of all the articles so consistency
 holds.
\end_layout

\begin_layout Enumerate
Ignorability - we assume that the effect of the hidden confounders is negligible
 or have a causal effect on the outcome only through the treatment.
 We assume that all the signifcant confounders are measured.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/corr_matrix.png
	scale 24

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Correlation between features
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Data distributions
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/weekend_count.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Number of articles published on weekend
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/data_channel_count.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Number of articles per data channel
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/data/num_shares_percentile_97.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Histogram of percentile 0.97 of shares count
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
We tried to estimate the average treatment effect (ATE) using a few methods:
 Inverse probability weighting (IPW), Matching, S learner, T learner and
 X learner.
 In order to estimate ATE we need that common support will hold.
 We calculated propensity scores (
\begin_inset Formula $e\left(x\right)=p\left(T=1\mid x\right)$
\end_inset

) in a few methods and eliminated some articles using the propensity histogram
 in order for common support to hold.
 We'll go into details in the following sections.
\end_layout

\begin_layout Subsubsection*
Propensity estimation
\end_layout

\begin_layout Standard
We estimated propensity scores with 3 methods:
\end_layout

\begin_layout Enumerate
Logistic regression: it had 
\begin_inset Formula $AUROC=0.596$
\end_inset

, which is good (close to 0.5) because it means the model is not predicting
 the treatment based on the confounders which would have violated common
 support.
 The calibration curve is mostly good - it turns bad in the graphs' edges,
 where there are very few samples.
 We trimmed the propensity histogram in order to have common support.
 (Figure 3)
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_normal.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_calibration.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_trimmed.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/log_trimmed_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Random forest: it had 
\begin_inset Formula $AUROC=0.7$
\end_inset

, which is good.
 In order to prevent overfitting we set hypermeters max tree depth to 7
 and minimum leaf samples to 40.
 The calibration curve is worse than the other methods.
 (Figure 4)
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_normal.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_calibration.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_trimmed.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/random_forest_trimmed_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Gradient boosting: it had 
\begin_inset Formula $AUROC=0.69$
\end_inset

, which is good.
 In order to prevent overfitting we set maximum tree depth to 3, minimum
 leaf samples to 30 and number of estimators to 100 .
 The calibration curve is quite good.
 It turns bad in the areas with less samples, but we trim these areas.
 (Figure 5)
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression propensity
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_normal.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propenstiy histogram log scale
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_calibration.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Calibration curve
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_trimmed.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity histogram after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/propensity/boosting_trimmed_log.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Propensity log scale after trimming
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We used recursive feature elimination (RFE) in order to find the most important
 features.
 We can see (figure 4) that logistic regression gives more wieght to the
 sentiment, subjectivity of the article and the ratio of positive and negative
 words, while random forest and gradient boosting give more weight to the
 length of the article, the number of elements of each type and the sentiment.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Feature importance rankings
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/log.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression model
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/random_forest.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Random forest model
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/feature_ranking/boosting.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Gradient boosting model
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Common support trimming
\end_layout

\begin_layout Standard
Our trimming method looked for the largest consecutive area in the propensity
 scores histogram, such that the sample count in each bin is higher than
 15 for both treatment types.
 With all the propensity estimation methods about 800 samples were eliminated.
 Random forest and boosting methods had about 400 common eliminated samples,
 while logistic regression propensity and either boosting or random forest
 propensity had only about 80 common eliminated samples.
 
\end_layout

\begin_layout Standard
We can see in the trimmed graphs that most of the trimmed data under logistic
 regression propensity and random forest propensity has scores between 0.3
 and 0.5.
 It means in these cases that the trimmed articles' more important features
 should be at or close to the intersection of the histograms of these features
 grouped by 
\begin_inset Formula $T$
\end_inset

.
 That is, it's harder to tell based on the article's more important features
 whether it was published on weekend or during the week.
 The larger intersection between the eliminated samples of random forest
 and boosting can be explained in that both have almost the same features'
 importance ranking.
\end_layout

\begin_layout Subsubsection*
ATE estimation
\end_layout

\begin_layout Standard
We estimated ATE using a couple of methods, together with each of the above
 propensity methods.
 
\end_layout

\begin_layout Enumerate
Inverse probability weighting (IPW)
\end_layout

\begin_layout Enumerate
Matching - we used oclidean distance as the distance metric in the algorithm.
 
\end_layout

\begin_layout Enumerate
S learner - we used gradient boosting model to estimate 
\begin_inset Formula $y\sim f(x,t)$
\end_inset

.
\end_layout

\begin_layout Enumerate
T learner - we used 2 gradient boosting models to estimate 
\begin_inset Formula $y_{0}\sim f_{0}(x)$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $y_{1}\sim f_{1}(x)$
\end_inset

.
\end_layout

\begin_layout Enumerate
X learner - in our dataset there are many units with 
\begin_inset Formula $T=0$
\end_inset

 (~34k samples) and only ~5k samples with 
\begin_inset Formula $T=1$
\end_inset

, and X learner is good at such cases of treated and control data size differenc
e.
 We model 
\begin_inset Formula $Y_{0}$
\end_inset

 and 
\begin_inset Formula $Y_{1}$
\end_inset

separately using 2 gradient boosting models.
 For CATE estimation we used propensity scores as weighting function.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The ATE estimates we got for every method and propensity score model are
 given in Figure 7.
 Few observations:
\end_layout

\begin_layout Itemize
IPW results are lower than the other methods under all the propensity methods
 we tried.
 However, modify propensity score method also changed IPW estimation significant
ly.
 When using logistic regression, IPW ATE was the closest for other method
 values.
 
\end_layout

\begin_layout Itemize
Changing propensity score method had low impact on Matching, S learner,
 T learner and X learner results.
 However, the propensity affects the 3 first methods only thourgh common
 support trimming, while on the latter it affects also through the ATE estimatio
n method.
 
\end_layout

\begin_layout Itemize
Highest ATE value is always estimated by Matching method.
 
\end_layout

\begin_layout Itemize
When training the learner models on a train set (75% of the data), we achieved
 
\begin_inset Formula $R^{2}$
\end_inset

 score of about 
\begin_inset Formula $-0.05$
\end_inset

 on the test set, which is relatively low.
 On the train set, S learner achieved score around 
\begin_inset Formula $0.35$
\end_inset

 (
\begin_inset Formula $y\sim f(x,t)$
\end_inset

) while T learner and X learner achieved 0.34 for 
\begin_inset Formula $y_{0}\sim f_{0}(x)$
\end_inset

 and 
\begin_inset Formula $0.61$
\end_inset

 for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $y_{1}\sim f_{1}(x)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ATE estimates
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/logistic_regression_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With logistic regression propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/random_forest_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With random forest propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/ate_estimates/boosting_propensity.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
With boosting propensity scores
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Weaknesses
\end_layout

\begin_layout Itemize
In our dataset there are only 5190 articles published on weekend.
 We need that common support will hold, so we are limited with the number
 of confounders we can consider in order to calculate ATE.
 Having more articles published on weekend could potentially change our
 results and impact our conclusions.
 
\end_layout

\begin_layout Itemize
We may have hidden confounders that their impact on the number of shares
 is not negligible.
\end_layout

\begin_layout Itemize
The dataset contains features that ought to be calculated from the articles'
 text in a method unknown to us, for example article's subjectivity level,
 sentiment polarity.
 We had to assume that these features well represent the articles' text.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Based on the results we got, articles published on weekend are getting more
 shares which indicates that they are more popular.
 According to ATE estimation range, articles published on weekend can get
 180-750 more shares comparing to week day.
 This is a significant amount of potential shares considering the fact that
 the median shares value in our dataset is 1400 and that the mean shares
 value is 3392.
 However, the hidden confounders might have a more signifcant effect on
 the number of shares.
 We believe that one signifcant hidden confounder is the article promotion
 algorithm in Mashable.
 Since the company knows its algorithm, it could shed more light on the
 signifcance of this confounder if they were conducting the data collection
 and the causal effect analysis.
 
\end_layout

\end_body
\end_document
