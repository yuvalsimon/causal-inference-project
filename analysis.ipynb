{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinate-earth",
   "metadata": {},
   "source": [
    "#### Features : \n",
    "* url: URL of the article (non-predictive)\n",
    "* timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "* n_tokens_title: Number of words in the title\n",
    "* n_tokens_content: Number of words in the content\n",
    "* n_unique_tokens: Rate of unique words in the content\n",
    "* n_non_stop_words: Rate of non-stop words in the content\n",
    "* n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "* num_hrefs: Number of links\n",
    "* num_self_hrefs: Number of links to other articles published by Mashable\n",
    "* num_imgs: Number of images\n",
    "* num_videos: Number of videos\n",
    "* average_token_length: Average length of the words in the content\n",
    "* num_keywords: Number of keywords in the metadata\n",
    "* data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "* data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "* data_channel_is_bus: Is data channel 'Business'?\n",
    "* data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "* data_channel_is_tech: Is data channel 'Tech'?\n",
    "* data_channel_is_world: Is data channel 'World'?\n",
    "* kw_min_min: Worst keyword (min. shares)\n",
    "* kw_max_min: Worst keyword (max. shares)\n",
    "* kw_avg_min: Worst keyword (avg. shares)\n",
    "* kw_min_max: Best keyword (min. shares)\n",
    "* kw_max_max: Best keyword (max. shares)\n",
    "* kw_avg_max: Best keyword (avg. shares)\n",
    "* kw_min_avg: Avg. keyword (min. shares)\n",
    "* kw_max_avg: Avg. keyword (max. shares)\n",
    "* kw_avg_avg: Avg. keyword (avg. shares)\n",
    "* self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "* self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "* self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "* weekday_is_monday: Was the article published on a Monday?\n",
    "* weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "* weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "* weekday_is_thursday: Was the article published on a Thursday?\n",
    "* weekday_is_friday: Was the article published on a Friday?\n",
    "* weekday_is_saturday: Was the article published on a Saturday?\n",
    "* weekday_is_sunday: Was the article published on a Sunday?\n",
    "* is_weekend: Was the article published on the weekend?\n",
    "* LDA_00: Closeness to LDA topic 0\n",
    "* LDA_01: Closeness to LDA topic 1\n",
    "* LDA_02: Closeness to LDA topic 2\n",
    "* LDA_03: Closeness to LDA topic 3\n",
    "* LDA_04: Closeness to LDA topic 4\n",
    "* global_subjectivity: Text subjectivity\n",
    "* global_sentiment_polarity: Text sentiment polarity\n",
    "* global_rate_positive_words: Rate of positive words in the content\n",
    "* global_rate_negative_words: Rate of negative words in the content\n",
    "* rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "* rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "* avg_positive_polarity: Avg. polarity of positive words\n",
    "* min_positive_polarity: Min. polarity of positive words\n",
    "* max_positive_polarity: Max. polarity of positive words\n",
    "* avg_negative_polarity: Avg. polarity of negative words\n",
    "* min_negative_polarity: Min. polarity of negative words\n",
    "* max_negative_polarity: Max. polarity of negative words\n",
    "* title_subjectivity: Title subjectivity\n",
    "* title_sentiment_polarity: Title polarity\n",
    "* abs_title_subjectivity: Absolute subjectivity level\n",
    "* abs_title_sentiment_polarity: Absolute polarity level\n",
    "* shares: Number of shares (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-humanitarian",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_basic_data_cleaning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_basic_data_cleaning:\n",
    "    df = pd.read_csv('onlinenews.csv')\n",
    "    df.columns = df.columns.map(lambda x: x.strip())\n",
    "    df = df.rename(columns={'self_reference_avg_sharess':'self_reference_avg_shares'})\n",
    "else:\n",
    "    df = pd.read_csv('onlinenews_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_channel(url):\n",
    "    page = requests.get(df.loc[1]['url'])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup.select('hgroup[data-channel]>h2')[0].get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_basic_data_cleaning:\n",
    "    # date column\n",
    "    df['date'] = df['url'].map(lambda x: '/'.join(x.split('/')[3:6][::-1]))\n",
    "    \n",
    "    # unify weekday columns\n",
    "    df['weekday'] = 0\n",
    "    for i, day in enumerate(['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']):\n",
    "        df['weekday'] += (i + 1) * df[f'weekday_is_{day}']\n",
    "    df = df.drop([i for i in df.columns if 'weekday_is' in i], axis=1)\n",
    "    \n",
    "    # replace data_channel_* features with single data_channel feature\n",
    "    df['data_channel'] = ''\n",
    "    data_channels = [i for i in df.columns if 'data_channel_' in i]\n",
    "    for c in data_channels:\n",
    "        df.loc[df[c] == 1,'data_channel'] = c.split('_')[-1]\n",
    "    df = df.drop(data_channels,axis=1)\n",
    "    \n",
    "    # get missing data_channel values\n",
    "    values = df[df['data_channel']=='']['data_channel'].copy()\n",
    "    for i in df[df['data_channel']==''].index:\n",
    "        try:\n",
    "            values.loc[i] = get_data_channel(df.loc[i,'url'])\n",
    "        except:\n",
    "            1\n",
    "    df.loc[df['data_channel']=='','data_channel'] = values\n",
    "\n",
    "    df.loc[21386,'data_channel'] = 'world'\n",
    "    df.loc[17003,'data_channel'] = 'entertainment'\n",
    "    df = df.drop(622).reset_index().drop('index', axis=1)\n",
    "    \n",
    "    df.loc[df['data_channel']=='business','data_channel'] = 'bus'\n",
    "    \n",
    "    df['is_weekend'] = df['is_weekend'].astype('int')\n",
    "    \n",
    "    # save to csv\n",
    "    df.to_csv('onlinenews_modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-accused",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_holiday\n",
    "\n",
    "# filter articles that were published on holidays\n",
    "df = df[df['date'].map(is_holiday) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "        'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
    "        'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'is_weekend',\n",
    "        'global_subjectivity', 'title_subjectivity', 'title_sentiment_polarity',\n",
    "        'global_sentiment_polarity', 'rate_positive_words', 'rate_negative_words',\n",
    "        'data_channel', 'shares']\n",
    "df = df[cols]\n",
    "t_label = 'is_weekend'\n",
    "y_label = 'shares'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-czech",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.5\n",
    "percentile_value = df[y_label].quantile(percentile)\n",
    "print(f'Percentile value: {percentile_value:.0f}')\n",
    "print(f'Max value: {df[\"shares\"].max()}' )\n",
    "print(f'Mean value: {df[\"shares\"].mean()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[y_label] < percentile_value][y_label].hist(bins=50)\n",
    "plt.xlabel('shares')\n",
    "plt.ylabel('number of articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-novelty",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,13))\n",
    "sns.heatmap(df.corr(method='pearson'), cmap='vlag', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-gothic",
   "metadata": {},
   "source": [
    "### data-channel counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('data_channel')['data_channel'].count().plot(kind='bar')\n",
    "plt.title('Channel counts')\n",
    "plt.ylabel('number of articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-ozone",
   "metadata": {},
   "source": [
    "## Weekend vs during week row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['during week', 'on weekend'], y=df[t_label].value_counts().values)\n",
    "plt.ylabel(\"number of articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-search",
   "metadata": {},
   "source": [
    "# Preperation for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from utils import factorize, propensity_func, trim_common_support\n",
    "\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import ate_estimators\n",
    "importlib.reload(ate_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-reference",
   "metadata": {},
   "source": [
    "# Propensity estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop([t_label, y_label], axis=1)\n",
    "t = df[t_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_estimators = {\n",
    "    \"log\": propensity_func(df, solver='newton-cg', penalty='none', max_iter=250),\n",
    "    \"random_forest\":  propensity_func(df, method='random_forest', max_depth=7, \\\n",
    "                                min_samples_leaf=40),\n",
    "    \"boosting\": propensity_func(df, method='boosting', \\\n",
    "                                n_estimators=100, max_depth=3, min_samples_leaf=30),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-progress",
   "metadata": {},
   "source": [
    "We'll now evaluate the propensity estimators and we'll try to achieve common support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"auroc:\")\n",
    "for method, estimator in propensity_estimators.items():\n",
    "    print(f\"  {method:<15}: {roc_auc_score(t, estimator.func(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = df.copy()\n",
    "fig, axs = plt.subplots(ncols=len(propensity_estimators), figsize=(15,5))\n",
    "plt.suptitle(\"Propensity with normal scale\")\n",
    "fig_log, axs_log = plt.subplots(ncols=len(propensity_estimators), figsize=(15,5))\n",
    "plt.suptitle(\"Propensity with log scale\")\n",
    "fig_cal, axs_cal = plt.subplots(ncols=len(propensity_estimators), figsize=(15,5))\n",
    "plt.suptitle(\"Calibration\")\n",
    "for i, key in enumerate(propensity_estimators):\n",
    "    propensity_scores = propensity_estimators[key].func(x)\n",
    "    df_prop['propensity'] = propensity_scores\n",
    "    sns.histplot(df_prop, x='propensity', bins=50, hue=t_label, ax=axs[i])\n",
    "    axs[i].set_xlabel('propensity score')\n",
    "    axs[i].set_ylabel('number of articles')\n",
    "    axs[i].set_title(key)\n",
    "    sns.histplot(df_prop, x='propensity', bins=50, hue=t_label, ax=axs_log[i])\n",
    "    axs_log[i].set_xlabel('propensity score')\n",
    "    axs_log[i].set_ylabel('number of articles')\n",
    "    axs_log[i].set_title(key)\n",
    "    axs_log[i].set_yscale('log')\n",
    "    xx, yy = calibration_curve(df_prop[t_label], df_prop['propensity'], n_bins = 20) \n",
    "    axs_cal[i].plot(yy, xx, marker = '.', label = 'Support Vector Classifier')\n",
    "    axs_cal[i].plot([0, 1], [0, 1], linestyle='--')\n",
    "    axs_cal[i].plot(yy, xx, marker='.')\n",
    "    axs_cal[i].set_title(key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in sorted(zip(x.columns.values, propensity_estimators[\"log\"].model.ranking_), key=lambda x: x[1]):\n",
    "#     print(f\"{i:25}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_method = \"boosting\"\n",
    "propensity_scores = propensity_estimators[propensity_method].func(x)\n",
    "df_prop = df.copy()\n",
    "df_prop['propensity'] = propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trimmed, df_rest1 = trim_common_support(df_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10,5))\n",
    "sns.histplot(df_trimmed, x='propensity', bins=50, hue=t_label, ax=axs[0])\n",
    "axs[0].set_xlabel('propensity score')\n",
    "axs[0].set_ylabel('number of articles')\n",
    "axs[0].set_title('normal scale')\n",
    "sns.histplot(df_trimmed, x='propensity', bins=50, hue=t_label, ax=axs[1])\n",
    "axs[1].set_xlabel('propensity score')\n",
    "axs[1].set_ylabel('number of articles')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_title('log scale')\n",
    "plt.suptitle(f'propensity histogram after trimming, with {propensity_method}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-movement",
   "metadata": {},
   "source": [
    "# ATE estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ate_estimators import ipw_ate, matching_ate, s_learner_ate, t_learner_ate, \\\n",
    "    x_learner_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_prop = df_trimmed.drop('propensity', axis=1)\n",
    "ates = pd.DataFrame(\n",
    "    dict(\n",
    "        ipw_ate=ipw_ate(df_no_prop, df_trimmed['propensity']),\n",
    "        matching_ate=matching_ate(df_no_prop),\n",
    "        s_learner_ate=s_learner_ate(df_no_prop, max_depth=7, min_samples_leaf=60, n_estimators=500),\n",
    "        t_learner_ate=t_learner_ate(df_no_prop, max_depth=7, min_samples_leaf=60, n_estimators=500),\n",
    "        x_learner_ate=x_learner_ate(df_no_prop, df_trimmed['propensity'], max_depth=7, min_samples_leaf=60, n_estimators=500),\n",
    "    ).items(),\n",
    "    columns=['Type', 'ATE']\n",
    ")\n",
    "ates.set_index('Type')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
