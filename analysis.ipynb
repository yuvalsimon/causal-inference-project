{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graduate-quantum",
   "metadata": {},
   "source": [
    "## Attribute Description :\n",
    "\n",
    "#### Input variables : \n",
    "* url: URL of the article (non-predictive)\n",
    "* timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "* n_tokens_title: Number of words in the title\n",
    "* n_tokens_content: Number of words in the content\n",
    "* n_unique_tokens: Rate of unique words in the content\n",
    "* n_non_stop_words: Rate of non-stop words in the content\n",
    "* n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "* num_hrefs: Number of links\n",
    "* num_self_hrefs: Number of links to other articles published by Mashable\n",
    "* num_imgs: Number of images\n",
    "* num_videos: Number of videos\n",
    "* average_token_length: Average length of the words in the content\n",
    "* num_keywords: Number of keywords in the metadata\n",
    "* data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "* data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "* data_channel_is_bus: Is data channel 'Business'?\n",
    "* data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "* data_channel_is_tech: Is data channel 'Tech'?\n",
    "* data_channel_is_world: Is data channel 'World'?\n",
    "* kw_min_min: Worst keyword (min. shares)\n",
    "* kw_max_min: Worst keyword (max. shares)\n",
    "* kw_avg_min: Worst keyword (avg. shares)\n",
    "* kw_min_max: Best keyword (min. shares)\n",
    "* kw_max_max: Best keyword (max. shares)\n",
    "* kw_avg_max: Best keyword (avg. shares)\n",
    "* kw_min_avg: Avg. keyword (min. shares)\n",
    "* kw_max_avg: Avg. keyword (max. shares)\n",
    "* kw_avg_avg: Avg. keyword (avg. shares)\n",
    "* self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "* self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "* self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "* weekday_is_monday: Was the article published on a Monday?\n",
    "* weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "* weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "* weekday_is_thursday: Was the article published on a Thursday?\n",
    "* weekday_is_friday: Was the article published on a Friday?\n",
    "* weekday_is_saturday: Was the article published on a Saturday?\n",
    "* weekday_is_sunday: Was the article published on a Sunday?\n",
    "* is_weekend: Was the article published on the weekend?\n",
    "* LDA_00: Closeness to LDA topic 0\n",
    "* LDA_01: Closeness to LDA topic 1\n",
    "* LDA_02: Closeness to LDA topic 2\n",
    "* LDA_03: Closeness to LDA topic 3\n",
    "* LDA_04: Closeness to LDA topic 4\n",
    "* global_subjectivity: Text subjectivity\n",
    "* global_sentiment_polarity: Text sentiment polarity\n",
    "* global_rate_positive_words: Rate of positive words in the content\n",
    "* global_rate_negative_words: Rate of negative words in the content\n",
    "* rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "* rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "* avg_positive_polarity: Avg. polarity of positive words\n",
    "* min_positive_polarity: Min. polarity of positive words\n",
    "* max_positive_polarity: Max. polarity of positive words\n",
    "* avg_negative_polarity: Avg. polarity of negative words\n",
    "* min_negative_polarity: Min. polarity of negative words\n",
    "* max_negative_polarity: Max. polarity of negative words\n",
    "* title_subjectivity: Title subjectivity\n",
    "* title_sentiment_polarity: Title polarity\n",
    "* abs_title_subjectivity: Absolute subjectivity level\n",
    "* abs_title_sentiment_polarity: Absolute polarity level\n",
    "* shares: Number of shares (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-optimization",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_basic_data_cleaning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_basic_data_cleaning:\n",
    "    df = pd.read_csv('onlinenews.csv')\n",
    "    df.columns = df.columns.map(lambda x: x.strip())\n",
    "    df = df.rename(columns={'self_reference_avg_sharess':'self_reference_avg_shares'})\n",
    "else:\n",
    "    df = pd.read_csv('onlinenews_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_channel(url):\n",
    "    page = requests.get(df.loc[1]['url'])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup.select('hgroup[data-channel]>h2')[0].get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_basic_data_cleaning:\n",
    "    # date column\n",
    "    df['date'] = df['url'].map(lambda x: '/'.join(x.split('/')[3:6][::-1]))\n",
    "    \n",
    "    # unify weekday columns\n",
    "    df['weekday'] = 0\n",
    "    for i, day in enumerate(['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']):\n",
    "        df['weekday'] += (i + 1) * df[f'weekday_is_{day}']\n",
    "    df = df.drop([i for i in df.columns if 'weekday_is' in i], axis=1)\n",
    "    \n",
    "    # replace data_channel_* features with single data_channel feature\n",
    "    df['data_channel'] = ''\n",
    "    data_channels = [i for i in df.columns if 'data_channel_' in i]\n",
    "    for c in data_channels:\n",
    "        df.loc[df[c] == 1,'data_channel'] = c.split('_')[-1]\n",
    "    df = df.drop(data_channels,axis=1)\n",
    "    \n",
    "    # get missing data_channel values\n",
    "    values = df[df['data_channel']=='']['data_channel'].copy()\n",
    "    for i in df[df['data_channel']==''].index:\n",
    "        try:\n",
    "            values.loc[i] = get_data_channel(df.loc[i,'url'])\n",
    "        except:\n",
    "            1\n",
    "    df.loc[df['data_channel']=='','data_channel'] = values\n",
    "\n",
    "    df.loc[21386,'data_channel'] = 'world'\n",
    "    df.loc[17003,'data_channel'] = 'entertainment'\n",
    "    df = df.drop(622).reset_index().drop('index', axis=1)\n",
    "    \n",
    "    df.loc[df['data_channel']=='business','data_channel'] = 'bus'\n",
    "    \n",
    "    # save to csv\n",
    "    df.to_csv('onlinenews_modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-norfolk",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "#         'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
    "#         'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'is_weekend',\n",
    "#         'global_subjectivity', 'title_subjectivity', 'title_sentiment_polarity',\n",
    "#         'global_sentiment_polarity', 'rate_positive_words', 'rate_negative_words',\n",
    "#         'data_channel', 'shares']\n",
    "cols = ['n_tokens_title', 'n_tokens_content', 'num_imgs', 'num_videos', 'num_keywords', 'is_weekend',\n",
    "        'global_subjectivity', 'title_subjectivity', 'title_sentiment_polarity',\n",
    "        'global_sentiment_polarity', 'rate_positive_words',\n",
    "        'data_channel', 'shares']\n",
    "df = df[cols]\n",
    "t_label = 'is_weekend'\n",
    "y_label = 'shares'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-renaissance",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.99\n",
    "percentile_value = df[y_label].quantile(0.99)\n",
    "print(f'Percentile value: {percentile_value:.0f}')\n",
    "print(f'Max value: {df[\"shares\"].max()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[y_label] < percentile_value][y_label].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-memorabilia",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(method='pearson'), cmap='vlag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-narrative",
   "metadata": {},
   "source": [
    "### data-channel counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('data_channel')['data_channel'].count().plot(kind='bar')\n",
    "plt.title('Channel counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-position",
   "metadata": {},
   "source": [
    "## Weekend vs during week row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[t_label].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-whole",
   "metadata": {},
   "source": [
    "# Preperation for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import factorize, propensity_func, trim_common_support, \\\n",
    "    balance_weights_for_histplot\n",
    "\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-secretary",
   "metadata": {},
   "source": [
    "# Propensity estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "df_train, df_test = train_test_split(df, test_size=test_size)\n",
    "x = df.drop([t_label, y_label], axis=1)\n",
    "x_train = df_train.drop([t_label, y_label], axis=1)\n",
    "x_test = df_test.drop([t_label, y_label], axis=1)\n",
    "t = df[t_label]\n",
    "t_train = df_train[t_label]\n",
    "t_test = df_test[t_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_estimators = {\n",
    "    \"log\": propensity_func(df_train, solver='liblinear', penalty='l2'),\n",
    "    \"random_forest\":  propensity_func(df_train, method='random_forest', max_depth=7, \\\n",
    "                                min_samples_leaf=5),\n",
    "    \"boosting\": propensity_func(df_train, method='boosting', learning_rate=0.015, \\\n",
    "                                n_estimators=400, max_depth=7),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"auroc:\")\n",
    "for method, estimator in propensity_estimators.items():\n",
    "    print(f\"  {method:<15}: {roc_auc_score(t_test, estimator(x_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_scores = propensity_estimators[\"boosting\"](x)\n",
    "df['propensity'] = propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='propensity', bins=20, hue=t_label)\n",
    "plt.xlabel('propensity score')\n",
    "plt.ylabel('number of articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trim_common_support(df, t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='propensity', hue=t_label,bins=20, weights=balance_weights_for_histplot(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-circuit",
   "metadata": {},
   "source": [
    "# ATE estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ate_estimators import ipw_ate, matching_ate, s_learner_ate, t_learner_ate, \\\n",
    "    x_learner_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_prop = df.drop('propensity', axis=1)\n",
    "ates = pd.DataFrame(\n",
    "    dict(\n",
    "        ipw_ate=ipw_ate(df_no_prop, df['propensity']),\n",
    "        matching_ate=matching_ate(df_no_prop),\n",
    "        s_learner_ate=s_learner_ate(df_no_prop),\n",
    "        t_learner_ate=t_learner_ate(df_no_prop),\n",
    "        x_learner_ate=x_learner_ate(df_no_prop, df['propensity']),\n",
    "    ).items(),\n",
    "    columns=['Type', 'ATE']\n",
    ")\n",
    "ates.set_index('Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-substance",
   "metadata": {},
   "source": [
    "# ### old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-sapphire",
   "metadata": {},
   "source": [
    "### Weekday histogram per data channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for i in df['data_channel'].unique():\n",
    "    counts[i] = df[df['data_channel']==i].groupby('weekday')[y_label].count()\n",
    "    counts[i].plot(kind='bar', title=i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(counts).corr(), annot=True)\n",
    "plt.title('Week-day number of shares correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_tokens_title', 'n_tokens_content', 'n_unique_words',\n",
    "        'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
    "        'num_imgs', 'num_videos', 'average_token_length', 'num_keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['self_reference_avg_shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shares_per_article = df.groupby(['data_channel','weekday'])[y_label].sum() /  df.groupby(['data_channel','weekday'])[y_label].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_avg_shares_per_article = avg_shares_per_article / avg_shares_per_article.groupby('data_channel').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = pd.DataFrame(norm_avg_shares_per_article).pivot_table(values=y_label,index='data_channel',columns='weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(vals*100, annot=True)\n",
    "plt.title('Week-day number of shares')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
